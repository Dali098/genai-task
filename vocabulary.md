# Vocabulary

## 1. Generative AI
AI that can make new stuff (text, images, etc). Not just “recognize things”.

## 2. LLM
Big language model trained on tons of text. Basically predicts what comes next.

## 3. Prompt
Your message to the AI. Sometimes even small wording changes the result a lot.

## 4. Prompt engineering
Trying different ways to ask the model so it gives something useful.

## 5. Token
Tiny piece of text. Models read everything in tokens, not full words.

## 6. Summarization
Short version of a long text. Just the main ideas, nothing extra.

## 7. Machine translation
Automatic translation between languages (Google, DeepL, etc).

## 8. Context window
How much text the model can “keep in mind” at once. If it’s small, it forgets stuff.

## 9. Hallucination
When AI invents facts or details that don’t exist. Happens more often than expected.

## 10. Dataset
All the data used to train a model. Quality of data = quality of model.

## 11. Diffusion models
Used for image generation. Start from random noise → slowly build a picture.

## 12. Bias
When outputs are skewed or unfair because of the training data.

## 13. Fine-tuning
Extra training on a smaller dataset to make the model better for one job.

## 14. Latency
Delay before the AI answers. Sometimes small, sometimes pretty noticeable.

## 15. RAG
AI that also searches info before answering. Helps avoid hallucinations a bit.

## 16. Parameters
Numbers inside the model that define how it behaves. More params ≠ always better, but often yes.

## 17. Inference
The moment when the model actually generates the output.

## 18. Transformer
Neural net architecture used in GPT/Claude/etc. Works well for text.

## 19. Embeddings
Numeric vectors that represent meaning of words or sentences. Models use them for search/similarity.

## 20. Temperature
A setting that changes how “creative” or “random” the output will be.
